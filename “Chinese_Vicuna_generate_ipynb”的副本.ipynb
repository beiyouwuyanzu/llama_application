{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "490ca015e5c746029b1e8afa64d6bfda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6bb4213aaab1454e898e07693aaf3b77",
              "IPY_MODEL_c1f441d3f0c44d03ac6f847fefd9251a",
              "IPY_MODEL_662a593aa73c4bcb9b58cd9fd4f3c44f"
            ],
            "layout": "IPY_MODEL_16b7b2d656ad423a82f2f8227ace63f5"
          }
        },
        "6bb4213aaab1454e898e07693aaf3b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d6560183d744f1891ba279835bf8c42",
            "placeholder": "​",
            "style": "IPY_MODEL_dff43be856b6478e8ad506115ee3a7f1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c1f441d3f0c44d03ac6f847fefd9251a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80777a869746490bb0b69f6afc16e7f0",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ed51b099d2145ce8306b9aac5470311",
            "value": 33
          }
        },
        "662a593aa73c4bcb9b58cd9fd4f3c44f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb0e184de07e4407b5883292a8d22597",
            "placeholder": "​",
            "style": "IPY_MODEL_5f03c574ac5b4204a3a6d4c96ff8d5ca",
            "value": " 33/33 [01:19&lt;00:00,  2.18s/it]"
          }
        },
        "16b7b2d656ad423a82f2f8227ace63f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d6560183d744f1891ba279835bf8c42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dff43be856b6478e8ad506115ee3a7f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80777a869746490bb0b69f6afc16e7f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ed51b099d2145ce8306b9aac5470311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb0e184de07e4407b5883292a8d22597": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f03c574ac5b4204a3a6d4c96ff8d5ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beiyouwuyanzu/llama_application/blob/main/%E2%80%9CChinese_Vicuna_generate_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Facico/Chinese-Vicuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_XEfY1e_Zz-",
        "outputId": "ef8f691b-b852-49f2-c65c-9c9c62b30277"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Chinese-Vicuna'...\n",
            "remote: Enumerating objects: 757, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 757 (delta 24), reused 23 (delta 14), pack-reused 713\u001b[K\n",
            "Receiving objects: 100% (757/757), 257.93 MiB | 11.35 MiB/s, done.\n",
            "Resolving deltas: 100% (395/395), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUcSeKN3-pYI"
      },
      "outputs": [],
      "source": [
        "!pip install -r ./Chinese-Vicuna/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LYsQk9y9byK",
        "outputId": "dbda68fb-554e-4244-f7b9-88060a96ae3c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "from peft import PeftModel, PeftModelForCausalLM, LoraConfig\n",
        "import transformers\n",
        "import gradio as gr\n",
        "import argparse\n",
        "import warnings\n",
        "import os\n",
        "import sys\n",
        "sys.path.insert(0, \"./Chinese-Vicuna\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G0EEVZmIoUz",
        "outputId": "0f6f78fe-26c8-4acb-cf7d-75019287a2a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('8013'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-dagfdloh2y7z --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_path = \"/content/drive/MyDrive/llama_data/checkpoint-200\"\n",
        "\n",
        "lora_bin_path = os.path.join(lora_path, \"adapter_model.bin\")\n",
        "print(lora_bin_path)\n",
        "if not os.path.exists(lora_bin_path) and 1:\n",
        "    pytorch_bin_path = os.path.join(lora_path, \"pytorch_model.bin\")\n",
        "    print(pytorch_bin_path)\n",
        "    if os.path.exists(pytorch_bin_path):\n",
        "        os.rename(pytorch_bin_path, lora_bin_path)\n",
        "        warnings.warn(\n",
        "            \"The file name of the lora checkpoint'pytorch_model.bin' is replaced with 'adapter_model.bin'\"\n",
        "        )\n",
        "    else:\n",
        "        assert ('Checkpoint is not Found!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Xt7ZlUoJNPV",
        "outputId": "c0feefd5-3af0-4645-eb4e-467d7d74940c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/llama_data/checkpoint-200/adapter_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 清除显存\n",
        "!apt install psmisc\n",
        "!sudo fuser /dev/nvidia*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A0rU7UeRkYq",
        "outputId": "1c786cda-891c-4859-9389-85c3d6047f29"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "psmisc is already the newest version (23.3-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "/dev/nvidia0:        13146m\n",
            "/dev/nvidiactl:      13146m\n",
            "/dev/nvidia-uvm:     13146m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !kill -9 13146"
      ],
      "metadata": {
        "id": "J9NHeIVZR4Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\"\"\"\n"
      ],
      "metadata": {
        "id": "imLxU3Qjb3LX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "from typing import Optional, Tuple, Union, List, Callable\n",
        "from transformers.generation.logits_process import LogitsProcessor\n",
        "from transformers.generation.beam_search import BeamSearchScorer\n",
        "from transformers.deepspeed import is_deepspeed_zero3_enabled\n",
        "from transformers.generation.utils import (\n",
        "    LogitsProcessorList,\n",
        "    StoppingCriteriaList,\n",
        "    GenerationConfig,\n",
        "    GenerationMixin,\n",
        ")\n",
        "import warnings\n",
        "from peft import PeftModel, PeftModelForCausalLM, LoraConfig\n",
        "import torch.distributed as dist\n",
        "from torch import nn\n",
        "import copy\n",
        "from accelerate.hooks import (\n",
        "    AlignDevicesHook,\n",
        "    add_hook_to_module,\n",
        "    remove_hook_from_submodules,\n",
        ")\n",
        "from accelerate.utils import get_balanced_memory\n",
        "from huggingface_hub import hf_hub_download\n",
        "from accelerate import dispatch_model, infer_auto_device_map\n",
        "from peft.utils import PeftType, set_peft_model_state_dict\n",
        "\n",
        "EPOCHS = 3  # we don't always need 3 tbh\n",
        "LEARNING_RATE = 3e-4  # the Karpathy constant\n",
        "CUTOFF_LEN = 256  # 256 accounts for about 96% of the data\n",
        "LORA_R = 8\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05\n",
        "VAL_SET_SIZE = 2000\n",
        "TARGET_MODULES = [\n",
        "    \"q_proj\",\n",
        "    \"v_proj\",\n",
        "]\n",
        "\n",
        "\n",
        "def printf(*args):\n",
        "    if os.environ.get('DEBUG',False):\n",
        "        print('>>> ', *args)\n",
        "\n",
        "class SteamGenerationMixin(PeftModelForCausalLM, GenerationMixin):\n",
        "    # support for streamly generation\n",
        "    # TODO: group_beam_search\n",
        "    @torch.no_grad()\n",
        "    def stream_generate(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        generation_config: Optional[GenerationConfig] = None,\n",
        "        logits_processor: Optional[LogitsProcessorList] = None,\n",
        "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
        "        prefix_allowed_tokens_fn: Optional[\n",
        "            Callable[[int, torch.Tensor], List[int]]\n",
        "        ] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self._reorder_cache = self.base_model._reorder_cache\n",
        "        if is_deepspeed_zero3_enabled() and dist.world_size() > 1:\n",
        "            synced_gpus = True\n",
        "        else:\n",
        "            synced_gpus = False\n",
        "\n",
        "        if kwargs.get(\"attention_mask\", None) is not None:\n",
        "            # concat prompt attention mask\n",
        "            prefix_attention_mask = torch.ones(\n",
        "                kwargs[\"input_ids\"].shape[0], self.peft_config.num_virtual_tokens\n",
        "            ).to(kwargs[\"input_ids\"].device)\n",
        "            kwargs[\"attention_mask\"] = torch.cat(\n",
        "                (prefix_attention_mask, kwargs[\"attention_mask\"]), dim=1\n",
        "            )\n",
        "        if kwargs.get(\"position_ids\", None) is not None:\n",
        "            warnings.warn(\n",
        "                \"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\"\n",
        "            )\n",
        "            kwargs[\"position_ids\"] = None\n",
        "        if kwargs.get(\"token_type_ids\", None) is not None:\n",
        "            warnings.warn(\n",
        "                \"Token type ids are not supported for parameter efficient tuning. Ignoring token type ids\"\n",
        "            )\n",
        "            kwargs[\"token_type_ids\"] = None\n",
        "\n",
        "        batch_size, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]\n",
        "\n",
        "        if generation_config is None:\n",
        "            generation_config = self.generation_config\n",
        "        generation_config = copy.deepcopy(generation_config)\n",
        "        model_kwargs = generation_config.update(**kwargs)\n",
        "\n",
        "        bos_token_id, eos_token_id, pad_token_id = (\n",
        "            generation_config.bos_token_id,\n",
        "            generation_config.eos_token_id,\n",
        "            generation_config.pad_token_id,\n",
        "        )\n",
        "\n",
        "        if isinstance(eos_token_id, int):\n",
        "            eos_token_id = [eos_token_id]\n",
        "\n",
        "        has_default_max_length = (\n",
        "            kwargs.get(\"max_length\") is None\n",
        "            and generation_config.max_length is not None\n",
        "        )\n",
        "        if has_default_max_length and generation_config.max_new_tokens is None:\n",
        "            warnings.warn(\n",
        "                f\"Using `max_length`'s default ({generation_config.max_length}) to control the generation length. \"\n",
        "                \"This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we\"\n",
        "                \" recommend using `max_new_tokens` to control the maximum length of the generation.\",\n",
        "                UserWarning,\n",
        "            )\n",
        "        elif generation_config.max_new_tokens is not None:\n",
        "            generation_config.max_length = (\n",
        "                generation_config.max_new_tokens + input_ids_seq_length\n",
        "            )\n",
        "        if generation_config.min_new_tokens is not None:\n",
        "            generation_config.min_length = (\n",
        "                generation_config.min_new_tokens + input_ids_seq_length\n",
        "            )\n",
        "\n",
        "        if input_ids_seq_length >= generation_config.max_length:\n",
        "            input_ids_string = (\n",
        "                \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n",
        "            )\n",
        "\n",
        "        # 2. Set generation parameters if not already defined\n",
        "        logits_processor = (\n",
        "            logits_processor if logits_processor is not None else LogitsProcessorList()\n",
        "        )\n",
        "        stopping_criteria = (\n",
        "            stopping_criteria\n",
        "            if stopping_criteria is not None\n",
        "            else StoppingCriteriaList()\n",
        "        )\n",
        "        # 7. determine generation mode\n",
        "        is_constraint_gen_mode = (\n",
        "            generation_config.constraints is not None or generation_config.force_words_ids is not None\n",
        "        )\n",
        "\n",
        "        is_contrastive_search_gen_mode = (\n",
        "            generation_config.top_k is not None\n",
        "            and generation_config.top_k > 1\n",
        "            and generation_config.do_sample is False\n",
        "            and generation_config.penalty_alpha is not None\n",
        "            and generation_config.penalty_alpha > 0\n",
        "        )\n",
        "\n",
        "        is_greedy_gen_mode = (\n",
        "            (generation_config.num_beams == 1)\n",
        "            and (generation_config.num_beam_groups == 1)\n",
        "            and generation_config.do_sample is False\n",
        "            and not is_constraint_gen_mode\n",
        "            and not is_contrastive_search_gen_mode\n",
        "        )\n",
        "        # beam=1 and do_sample=True\n",
        "        is_sample_gen_mode = (\n",
        "            (generation_config.num_beams == 1)\n",
        "            and (generation_config.num_beam_groups == 1)\n",
        "            and generation_config.do_sample is True\n",
        "            and not is_constraint_gen_mode\n",
        "            and not is_contrastive_search_gen_mode\n",
        "        )\n",
        "        is_beam_gen_mode = (\n",
        "            (generation_config.num_beams > 1)\n",
        "            and (generation_config.num_beam_groups == 1)\n",
        "            and generation_config.do_sample is False\n",
        "            and not is_constraint_gen_mode\n",
        "            and not is_contrastive_search_gen_mode\n",
        "        )\n",
        "        is_beam_sample_gen_mode = (\n",
        "            (generation_config.num_beams > 1)\n",
        "            and (generation_config.num_beam_groups == 1)\n",
        "            and generation_config.do_sample is True\n",
        "            and not is_constraint_gen_mode\n",
        "            and not is_contrastive_search_gen_mode\n",
        "        )\n",
        "        is_group_beam_gen_mode = (\n",
        "            (generation_config.num_beams > 1)\n",
        "            and (generation_config.num_beam_groups > 1)\n",
        "            and not is_constraint_gen_mode\n",
        "            and not is_contrastive_search_gen_mode\n",
        "        )\n",
        "        # 8. prepare distribution pre_processing samplers\n",
        "        logits_processor = self._get_logits_processor(\n",
        "            generation_config=generation_config,\n",
        "            input_ids_seq_length=input_ids_seq_length,\n",
        "            encoder_input_ids=input_ids,\n",
        "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
        "            logits_processor=logits_processor,\n",
        "        )\n",
        "        # 9. prepare stopping criteria\n",
        "        stopping_criteria = self._get_stopping_criteria(\n",
        "            generation_config=generation_config, stopping_criteria=stopping_criteria\n",
        "        )\n",
        "        logits_warper = self._get_logits_warper(generation_config)\n",
        "\n",
        "        if is_greedy_gen_mode:\n",
        "            # 11. run greedy search\n",
        "            return self.greedy_search(\n",
        "                input_ids,\n",
        "                logits_processor,\n",
        "                stopping_criteria,\n",
        "                generation_config,\n",
        "                synced_gpus,\n",
        "                **model_kwargs,\n",
        "            )\n",
        "        elif is_sample_gen_mode:\n",
        "            # 12. expand input_ids with `num_return_sequences` additional sequences per batch\n",
        "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
        "                input_ids=input_ids,\n",
        "                expand_size=generation_config.num_return_sequences,\n",
        "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
        "                **model_kwargs,\n",
        "            )\n",
        "            return self.sample(\n",
        "                generation_config,\n",
        "                input_ids,\n",
        "                logits_processor,\n",
        "                logits_warper,\n",
        "                stopping_criteria,\n",
        "                synced_gpus,\n",
        "                **model_kwargs,\n",
        "            )\n",
        "        elif is_beam_gen_mode:\n",
        "            return self.beam_search(\n",
        "                generation_config,\n",
        "                input_ids,\n",
        "                logits_processor,\n",
        "                stopping_criteria,\n",
        "                synced_gpus,\n",
        "                **model_kwargs,\n",
        "            )\n",
        "        elif is_beam_sample_gen_mode:\n",
        "            # interleave input_ids with `num_beams` additional sequences per batch\n",
        "            return self.beam_sample(\n",
        "                input_ids,\n",
        "                logits_processor,\n",
        "                logits_warper,\n",
        "                stopping_criteria,\n",
        "                generation_config,\n",
        "                synced_gpus,\n",
        "                **model_kwargs,\n",
        "            )\n",
        "        else:\n",
        "            raise Exception('not implement')\n",
        "        \n",
        "    def sample(\n",
        "        self,\n",
        "        generation_config,\n",
        "        input_ids,\n",
        "        logits_processor,\n",
        "        logits_warper,\n",
        "        stopping_criteria,\n",
        "        synced_gpus,\n",
        "        **model_kwargs,\n",
        "    ):\n",
        "        bos_token_id, eos_token_id, pad_token_id = (\n",
        "            generation_config.bos_token_id,\n",
        "            generation_config.eos_token_id,\n",
        "            generation_config.pad_token_id,\n",
        "        )\n",
        "        if isinstance(eos_token_id, int):\n",
        "            eos_token_id = [eos_token_id]\n",
        "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
        "        # keep track of which sequences are already finished\n",
        "        unfinished_sequences = torch.ones(input_ids.shape[0], dtype=torch.long, device=input_ids.device)\n",
        "        this_peer_finished = False  # used by synced_gpus only\n",
        "        scores=()\n",
        "        # auto-regressive generation\n",
        "        while True:\n",
        "            if synced_gpus:\n",
        "                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n",
        "                # The following logic allows an early break if all peers finished generating their sequence\n",
        "                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n",
        "                # send 0.0 if we finished, 1.0 otherwise\n",
        "                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
        "                # did all peers finish? the reduced sum will be 0.0 then\n",
        "                if this_peer_finished_flag.item() == 0.0:\n",
        "                    break\n",
        "            # prepare model inputs\n",
        "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "            # forward pass to get next token\n",
        "            outputs = self(\n",
        "                **model_inputs,\n",
        "                return_dict=True,\n",
        "            )\n",
        "            if synced_gpus and this_peer_finished:\n",
        "                continue  # don't waste resources running the code we don't need\n",
        "            next_token_logits = outputs.logits[:, -1, :]\n",
        "            # pre-process distribution\n",
        "            next_token_scores = logits_processor(input_ids, next_token_logits)\n",
        "            next_token_scores = logits_warper(input_ids, next_token_scores)\n",
        "\n",
        "            # sample\n",
        "            probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
        "            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
        "\n",
        "            # finished sentences should have their next token be a padding token\n",
        "            if eos_token_id is not None:\n",
        "                if pad_token_id is None:\n",
        "                    raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n",
        "                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
        "\n",
        "            # update generated ids, model inputs, and length for next step\n",
        "            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
        "            model_kwargs = self._update_model_kwargs_for_generation(\n",
        "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
        "            )\n",
        "            yield input_ids\n",
        "            # if eos_token was found in one sentence, set sentence to finished\n",
        "            if eos_token_id_tensor is not None:\n",
        "                unfinished_sequences = unfinished_sequences.mul(\n",
        "                    next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n",
        "                )\n",
        "            \n",
        "            # stop when each sentence is finished, or if we exceed the maximum length\n",
        "            if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n",
        "                if not synced_gpus:\n",
        "                    break\n",
        "                else:\n",
        "                    this_peer_finished = True\n",
        "        yield input_ids\n",
        "\n",
        "    def beam_sample(\n",
        "        self,\n",
        "        input_ids,\n",
        "        logits_processor,\n",
        "        logits_warper,\n",
        "        stopping_criteria,\n",
        "        generation_config,\n",
        "        synced_gpus,\n",
        "        **model_kwargs,\n",
        "    ):\n",
        "        bos_token_id, eos_token_id, pad_token_id = (\n",
        "            generation_config.bos_token_id,\n",
        "            generation_config.eos_token_id,\n",
        "            generation_config.pad_token_id,\n",
        "        )\n",
        "        if isinstance(eos_token_id, int):\n",
        "            eos_token_id = [eos_token_id]\n",
        "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
        "        num_beams = generation_config.num_beams\n",
        "        batch_size, cur_len = input_ids.shape[0], input_ids.shape[-1]\n",
        "        beam_scorer = BeamSearchScorer(\n",
        "            batch_size=batch_size,\n",
        "            num_beams=generation_config.num_beams,\n",
        "            device=input_ids.device,\n",
        "            length_penalty=generation_config.length_penalty,\n",
        "            do_early_stopping=generation_config.early_stopping,\n",
        "            num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
        "            max_length=generation_config.max_length,\n",
        "        )\n",
        "        input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
        "            input_ids=input_ids,\n",
        "            expand_size=generation_config.num_beams * generation_config.num_return_sequences,\n",
        "            is_encoder_decoder=self.config.is_encoder_decoder,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "        scores = ()\n",
        "        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
        "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
        "\n",
        "        this_peer_finished = False  # used by synced_gpus only\n",
        "        while True:\n",
        "            if synced_gpus:\n",
        "                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n",
        "                # The following logic allows an early break if all peers finished generating their sequence\n",
        "                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n",
        "                # send 0.0 if we finished, 1.0 otherwise\n",
        "                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
        "                # did all peers finish? the reduced sum will be 0.0 then\n",
        "                if this_peer_finished_flag.item() == 0.0:\n",
        "                    break\n",
        "\n",
        "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "            outputs = self(\n",
        "                **model_inputs,\n",
        "                return_dict=True,\n",
        "            )\n",
        "\n",
        "            if synced_gpus and this_peer_finished:\n",
        "                cur_len = cur_len + 1\n",
        "                continue  # don't waste resources running the code we don't need\n",
        "\n",
        "            next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            # hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`\n",
        "            # cannot be generated both before and after the `nn.functional.log_softmax` operation.\n",
        "            next_token_logits = self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len)\n",
        "            next_token_scores = nn.functional.log_softmax(\n",
        "                next_token_logits, dim=-1\n",
        "            )  # (batch_size * num_beams, vocab_size)\n",
        "\n",
        "            next_token_scores_processed = logits_processor(input_ids, next_token_scores)\n",
        "            next_token_scores = next_token_scores_processed + beam_scores[:, None].expand_as(next_token_scores)\n",
        "            # Note: logits warpers are intentionally applied after adding running beam scores. On some logits warpers\n",
        "            # (like top_p) this is indiferent, but on others (like temperature) it is not. For reference, see\n",
        "            # https://github.com/huggingface/transformers/pull/5420#discussion_r449779867\n",
        "            next_token_scores = logits_warper(input_ids, next_token_scores)\n",
        "\n",
        "            # reshape for beam search\n",
        "            vocab_size = next_token_scores.shape[-1]\n",
        "            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
        "\n",
        "            probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
        "\n",
        "            next_tokens = torch.multinomial(probs, num_samples=2 * num_beams)\n",
        "            next_token_scores = torch.gather(next_token_scores, -1, next_tokens)\n",
        "\n",
        "            next_token_scores, _indices = torch.sort(next_token_scores, descending=True, dim=1)\n",
        "            next_tokens = torch.gather(next_tokens, -1, _indices)\n",
        "\n",
        "            next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n",
        "            next_tokens = next_tokens % vocab_size\n",
        "\n",
        "            # stateless\n",
        "            beam_outputs = beam_scorer.process(\n",
        "                input_ids,\n",
        "                next_token_scores,\n",
        "                next_tokens,\n",
        "                next_indices,\n",
        "                pad_token_id=pad_token_id,\n",
        "                eos_token_id=eos_token_id,\n",
        "                beam_indices=None,\n",
        "            )\n",
        "            beam_scores = beam_outputs[\"next_beam_scores\"]\n",
        "            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
        "            beam_idx = beam_outputs[\"next_beam_indices\"]\n",
        "\n",
        "            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
        "            yield input_ids\n",
        "            model_kwargs = self._update_model_kwargs_for_generation(\n",
        "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
        "            )\n",
        "            if model_kwargs[\"past_key_values\"] is not None:\n",
        "                model_kwargs[\"past_key_values\"] = self._reorder_cache(model_kwargs[\"past_key_values\"], beam_idx)\n",
        "\n",
        "            # increase cur_len\n",
        "            cur_len = cur_len + 1\n",
        "\n",
        "            if beam_scorer.is_done or stopping_criteria(input_ids, scores):\n",
        "                if not synced_gpus:\n",
        "                    break\n",
        "                else:\n",
        "                    this_peer_finished = True\n",
        "\n",
        "        sequence_outputs = beam_scorer.finalize(\n",
        "            input_ids,\n",
        "            beam_scores,\n",
        "            next_tokens,\n",
        "            next_indices,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            max_length=stopping_criteria.max_length,\n",
        "            beam_indices=None,\n",
        "        )\n",
        "        yield sequence_outputs[\"sequences\"]\n",
        "\n",
        "    def greedy_search(\n",
        "        self,\n",
        "        input_ids,\n",
        "        logits_processor,\n",
        "        stopping_criteria,\n",
        "        generation_config,\n",
        "        synced_gpus,\n",
        "        **model_kwargs,\n",
        "    ):\n",
        "        # init values\n",
        "        bos_token_id, eos_token_id, pad_token_id = (\n",
        "            generation_config.bos_token_id,\n",
        "            generation_config.eos_token_id,\n",
        "            generation_config.pad_token_id,\n",
        "        )\n",
        "        if isinstance(eos_token_id, int):\n",
        "            eos_token_id = [eos_token_id]\n",
        "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
        "        # init attention / hidden states / scores tuples\n",
        "        scores = () \n",
        "        # keep track of which sequences are already finished\n",
        "        unfinished_sequences = torch.ones(input_ids.shape[0], dtype=torch.long, device=input_ids.device)\n",
        "        this_peer_finished = False  # used by synced_gpus only\n",
        "        while True:\n",
        "            if synced_gpus:\n",
        "                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n",
        "                # The following logic allows an early break if all peers finished generating their sequence\n",
        "                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n",
        "                # send 0.0 if we finished, 1.0 otherwise\n",
        "                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
        "                # did all peers finish? the reduced sum will be 0.0 then\n",
        "                if this_peer_finished_flag.item() == 0.0:\n",
        "                    break\n",
        "\n",
        "            # prepare model inputs\n",
        "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "            # forward pass to get next token\n",
        "            outputs = self(\n",
        "                **model_inputs,\n",
        "                return_dict=True,\n",
        "            )\n",
        "\n",
        "            if synced_gpus and this_peer_finished:\n",
        "                continue  # don't waste resources running the code we don't need\n",
        "\n",
        "            next_token_logits = outputs.logits[:, -1, :]\n",
        "            # pre-process distribution\n",
        "            next_tokens_scores = logits_processor(input_ids, next_token_logits)\n",
        "            # argmax\n",
        "            next_tokens = torch.argmax(next_tokens_scores, dim=-1)\n",
        "            # finished sentences should have their next token be a padding token\n",
        "            if eos_token_id is not None:\n",
        "                if pad_token_id is None:\n",
        "                    raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n",
        "                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
        "            # update generated ids, model inputs, and length for next step\n",
        "            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
        "            model_kwargs = self._update_model_kwargs_for_generation(\n",
        "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
        "            )\n",
        "            yield input_ids\n",
        "            # if eos_token was found in one sentence, set sentence to finished\n",
        "            if eos_token_id_tensor is not None:\n",
        "                unfinished_sequences = unfinished_sequences.mul(\n",
        "                    next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n",
        "                )\n",
        "\n",
        "            # stop when each sentence is finished, or if we exceed the maximum length\n",
        "            if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n",
        "                if not synced_gpus:\n",
        "                    break\n",
        "                else:\n",
        "                    this_peer_finished = True\n",
        "        yield input_ids\n",
        "\n",
        "    def beam_search(\n",
        "        self,\n",
        "        generation_config,\n",
        "        input_ids,\n",
        "        logits_processor,\n",
        "        stopping_criteria,\n",
        "        synced_gpus,\n",
        "        **model_kwargs,\n",
        "    ):\n",
        "        # 10. go into beam search generation modes\n",
        "        # 11. prepare beam search scorer\n",
        "        bos_token_id, eos_token_id, pad_token_id = (\n",
        "            generation_config.bos_token_id,\n",
        "            generation_config.eos_token_id,\n",
        "            generation_config.pad_token_id,\n",
        "        )\n",
        "        if isinstance(eos_token_id, int):\n",
        "            eos_token_id = [eos_token_id]\n",
        "        num_beams = generation_config.num_beams\n",
        "        batch_size, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]\n",
        "        beam_scorer = BeamSearchScorer(\n",
        "            batch_size=batch_size,\n",
        "            num_beams=generation_config.num_beams,\n",
        "            device=input_ids.device,\n",
        "            length_penalty=generation_config.length_penalty,\n",
        "            do_early_stopping=generation_config.early_stopping,\n",
        "            num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
        "            max_length=generation_config.max_length,\n",
        "        )\n",
        "        # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
        "        input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
        "            input_ids=input_ids,\n",
        "            expand_size=generation_config.num_beams,\n",
        "            is_encoder_decoder=self.config.is_encoder_decoder,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "        # beam_search logits\n",
        "        batch_beam_size, cur_len = input_ids.shape\n",
        "        if num_beams * batch_size != batch_beam_size:\n",
        "            raise ValueError(\n",
        "                f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n",
        "            )\n",
        "        beam_scores = torch.zeros(\n",
        "            (batch_size, num_beams), dtype=torch.float, device=input_ids.device\n",
        "        )\n",
        "        beam_scores[:, 1:] = -1e9\n",
        "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
        "        this_peer_finished = False  # used by synced_gpus only\n",
        "        while True:\n",
        "            if synced_gpus:\n",
        "                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n",
        "                # The following logic allows an early break if all peers finished generating their sequence\n",
        "                this_peer_finished_flag = torch.tensor(\n",
        "                    0.0 if this_peer_finished else 1.0\n",
        "                ).to(input_ids.device)\n",
        "                # send 0.0 if we finished, 1.0 otherwise\n",
        "                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n",
        "                # did all peers finish? the reduced sum will be 0.0 then\n",
        "                if this_peer_finished_flag.item() == 0.0:\n",
        "                    break\n",
        "\n",
        "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "            outputs = self(\n",
        "                **model_inputs,\n",
        "                return_dict=True,\n",
        "                output_attentions=False,\n",
        "                output_hidden_states=False,\n",
        "            )\n",
        "\n",
        "            if synced_gpus and this_peer_finished:\n",
        "                cur_len = cur_len + 1\n",
        "                continue  # don't waste resources running the code we don't need\n",
        "\n",
        "            next_token_logits = outputs.logits[:, -1, :]\n",
        "            # next_token_logits = self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len) hack: adjust tokens for Marian.\n",
        "            next_token_scores = nn.functional.log_softmax(\n",
        "                next_token_logits, dim=-1\n",
        "            )  # (batch_size * num_beams, vocab_size)\n",
        "            next_token_scores_processed = logits_processor(input_ids, next_token_scores)\n",
        "            next_token_scores = next_token_scores_processed + beam_scores[\n",
        "                :, None\n",
        "            ].expand_as(next_token_scores)\n",
        "\n",
        "            # reshape for beam search\n",
        "            vocab_size = next_token_scores.shape[-1]\n",
        "            next_token_scores = next_token_scores.view(\n",
        "                batch_size, num_beams * vocab_size\n",
        "            )\n",
        "\n",
        "            # Sample 2 next tokens for each beam (so we have some spare tokens and match output of beam search)\n",
        "            next_token_scores, next_tokens = torch.topk(\n",
        "                next_token_scores, 2 * num_beams, dim=1, largest=True, sorted=True\n",
        "            )\n",
        "            next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n",
        "            next_tokens = next_tokens % vocab_size\n",
        "            # stateless\n",
        "            beam_outputs = beam_scorer.process(\n",
        "                input_ids,\n",
        "                next_token_scores,\n",
        "                next_tokens,\n",
        "                next_indices,\n",
        "                pad_token_id=pad_token_id,\n",
        "                eos_token_id=eos_token_id,\n",
        "                beam_indices=None,\n",
        "            )\n",
        "            beam_scores = beam_outputs[\"next_beam_scores\"]\n",
        "            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
        "            beam_idx = beam_outputs[\"next_beam_indices\"]\n",
        "\n",
        "            input_ids = torch.cat(\n",
        "                [input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1\n",
        "            )\n",
        "            model_kwargs = self._update_model_kwargs_for_generation(\n",
        "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
        "            )\n",
        "            if model_kwargs[\"past_key_values\"] is not None:\n",
        "                model_kwargs[\"past_key_values\"] = self._reorder_cache(\n",
        "                    model_kwargs[\"past_key_values\"], beam_idx\n",
        "                )\n",
        "\n",
        "            # increase cur_len\n",
        "            cur_len = cur_len + 1\n",
        "\n",
        "            yield input_ids\n",
        "\n",
        "            if beam_scorer.is_done or stopping_criteria(input_ids, None):\n",
        "                if not synced_gpus:\n",
        "                    break\n",
        "                else:\n",
        "                    this_peer_finished = True\n",
        "\n",
        "        final_result = beam_scorer.finalize(\n",
        "            input_ids,\n",
        "            beam_scores,\n",
        "            next_tokens,\n",
        "            next_indices,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            max_length=stopping_criteria.max_length,\n",
        "            beam_indices=None,\n",
        "        )\n",
        "        yield final_result[\"sequences\"]\n",
        "\n",
        "    # default it call `model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](model, config)`, not cls!! so inherent PeftModelForCausalLM is no sense\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model, model_id, **kwargs):\n",
        "        print(model_id)\n",
        "        config = LoraConfig(\n",
        "            r=LORA_R,\n",
        "            lora_alpha=LORA_ALPHA,\n",
        "            target_modules=TARGET_MODULES,\n",
        "            lora_dropout=LORA_DROPOUT,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "        )\n",
        "        # load the config\n",
        "        # config = LoraConfig.from_pretrained(model_id)\n",
        "\n",
        "        if getattr(model, \"hf_device_map\", None) is not None:\n",
        "            remove_hook_from_submodules(model)\n",
        "\n",
        "        # here is the hack\n",
        "        model = cls(model, config)\n",
        "\n",
        "        # load weights if any\n",
        "        if os.path.exists(os.path.join(model_id, \"adapter_model.bin\")):\n",
        "            filename = os.path.join(model_id, \"adapter_model.bin\")\n",
        "        else:\n",
        "            try:\n",
        "                filename = hf_hub_download(model_id, \"adapter_model.bin\")\n",
        "            except:  # noqa\n",
        "                raise ValueError(\n",
        "                    f\"Can't find weights for {model_id} in {model_id} or in the Hugging Face Hub. \"\n",
        "                    f\"Please check that the file {'adapter_model.bin'} is present at {model_id}.\"\n",
        "                )\n",
        "\n",
        "        adapters_weights = torch.load(\n",
        "            filename,\n",
        "            map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        )\n",
        "        # load the weights into the model\n",
        "        model = set_peft_model_state_dict(model, adapters_weights)\n",
        "        if getattr(model, \"hf_device_map\", None) is not None:\n",
        "            device_map = kwargs.get(\"device_map\", \"auto\")\n",
        "            max_memory = kwargs.get(\"max_memory\", None)\n",
        "            no_split_module_classes = model._no_split_modules\n",
        "            if device_map != \"sequential\":\n",
        "                max_memory = get_balanced_memory(\n",
        "                    model,\n",
        "                    max_memory=max_memory,\n",
        "                    no_split_module_classes=no_split_module_classes,\n",
        "                    low_zero=(device_map == \"balanced_low_0\"),\n",
        "                )\n",
        "            if isinstance(device_map, str):\n",
        "                device_map = infer_auto_device_map(\n",
        "                    model,\n",
        "                    max_memory=max_memory,\n",
        "                    no_split_module_classes=no_split_module_classes,\n",
        "                )\n",
        "            model = dispatch_model(model, device_map=device_map)\n",
        "            hook = AlignDevicesHook(io_same_device=True)\n",
        "            if model.peft_config.peft_type == PeftType.LORA:\n",
        "                add_hook_to_module(model.base_model.model, hook)\n",
        "            else:\n",
        "                remove_hook_from_submodules(model.prompt_encoder)\n",
        "                add_hook_to_module(model.base_model, hook)\n",
        "        return model\n",
        "\n"
      ],
      "metadata": {
        "id": "PPtlb24dUwVa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
        "BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
        "LOAD_8BIT = True\n",
        "LORA_WEIGHTS = \"/content/drive/MyDrive/llama_data/checkpoint-200/\"\n",
        "device = \"cuda\"\n",
        "\n",
        "# if device == \"cuda\":\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    load_in_8bit=LOAD_8BIT,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "model = SteamGenerationMixin.from_pretrained(\n",
        "    model, LORA_WEIGHTS, torch_dtype=torch.float16, device_map={\"\": 0}\n",
        ")\n",
        "# elif device == \"mps\":\n",
        "#     model = LlamaForCausalLM.from_pretrained(\n",
        "#         BASE_MODEL,\n",
        "#         device_map={\"\": device},\n",
        "#         torch_dtype=torch.float16,\n",
        "#     )\n",
        "#     model = SteamGenerationMixin.from_pretrained(\n",
        "#         model,\n",
        "#         LORA_WEIGHTS,\n",
        "#         device_map={\"\": device},\n",
        "#         torch_dtype=torch.float16,\n",
        "#     )\n",
        "# else:\n",
        "#     model = LlamaForCausalLM.from_pretrained(\n",
        "#         BASE_MODEL, device_map={\"\": device}, low_cpu_mem_usage=True\n",
        "#     )\n",
        "#     model = SteamGenerationMixin.from_pretrained(\n",
        "#         model,\n",
        "#         LORA_WEIGHTS,\n",
        "#         device_map={\"\": device},\n",
        "#     )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "490ca015e5c746029b1e8afa64d6bfda",
            "6bb4213aaab1454e898e07693aaf3b77",
            "c1f441d3f0c44d03ac6f847fefd9251a",
            "662a593aa73c4bcb9b58cd9fd4f3c44f",
            "16b7b2d656ad423a82f2f8227ace63f5",
            "8d6560183d744f1891ba279835bf8c42",
            "dff43be856b6478e8ad506115ee3a7f1",
            "80777a869746490bb0b69f6afc16e7f0",
            "5ed51b099d2145ce8306b9aac5470311",
            "bb0e184de07e4407b5883292a8d22597",
            "5f03c574ac5b4204a3a6d4c96ff8d5ca"
          ]
        },
        "id": "S3y084KdJyfU",
        "outputId": "49f8e21b-7984-4701-92fb-dba4577d405d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "490ca015e5c746029b1e8afa64d6bfda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/llama_data/checkpoint-200/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    input,\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    top_k=40,\n",
        "    num_beams=4,\n",
        "    max_new_tokens=128,\n",
        "    min_new_tokens=1,\n",
        "    repetition_penalty=2.0,\n",
        "    **kwargs,\n",
        "):\n",
        "    prompt = generate_prompt(input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        num_beams=num_beams,\n",
        "        bos_token_id=1,\n",
        "        eos_token_id=2,\n",
        "        pad_token_id=0,\n",
        "        max_new_tokens=max_new_tokens, # max_length=max_new_tokens+input_sequence\n",
        "        min_new_tokens=min_new_tokens, # min_length=min_new_tokens+input_sequence\n",
        "        **kwargs,\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        if 1:\n",
        "            for generation_output in model.stream_generate(\n",
        "                input_ids=input_ids,\n",
        "                generation_config=generation_config,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=False,\n",
        "                repetition_penalty=float(repetition_penalty),\n",
        "            ):\n",
        "                outputs = tokenizer.batch_decode(generation_output)\n",
        "                show_text = \"\\n--------------------------------------------\\n\".join(\n",
        "                    [output.split(\"### Response:\")[1].strip().replace('�','')+\" ▌\" for output in outputs]\n",
        "                )\n",
        "                # if show_text== '':\n",
        "                #     yield last_show_text\n",
        "                # else:\n",
        "                yield show_text\n",
        "            yield outputs[0].split(\"### Response:\")[1].strip().replace('�','')\n",
        "        else:\n",
        "            generation_output = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                generation_config=generation_config,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=False,\n",
        "                repetition_penalty=1.3,\n",
        "            )\n",
        "            output = generation_output.sequences[0]\n",
        "            output = tokenizer.decode(output).split(\"### Response:\")[1].strip()\n",
        "            print(output)\n",
        "            yield output\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWuPGz_BWvUb",
        "outputId": "0cdc94bc-4392-4b73-f7d3-f914240da465"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
            "The class this function is called from is 'LlamaTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in evaluate(\"在一正方形花池的4周栽了44棵柳树，每两棵柳树之间的间隔是20米，这个正方形的周长=多少米？\"):\n",
        "    print(x)"
      ],
      "metadata": {
        "id": "liMYZfV1bkQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.Interface(\n",
        "    fn=evaluate,\n",
        "    inputs=[\n",
        "        gr.components.Textbox(\n",
        "            lines=2, label=\"Input\", placeholder=\"Tell me about alpacas.\"\n",
        "        ),\n",
        "        gr.components.Slider(minimum=0, maximum=1, value=0.1, label=\"Temperature\"),\n",
        "        gr.components.Slider(minimum=0, maximum=1, value=0.75, label=\"Top p\"),\n",
        "        gr.components.Slider(minimum=0, maximum=100, step=1, value=40, label=\"Top k\"),\n",
        "        gr.components.Slider(minimum=1, maximum=10, step=1, value=4, label=\"Beams Number\"),\n",
        "        gr.components.Slider(\n",
        "            minimum=1, maximum=2000, step=1, value=256, label=\"Max New Tokens\"\n",
        "        ),\n",
        "        gr.components.Slider(\n",
        "            minimum=1, maximum=300, step=1, value=1, label=\"Min New Tokens\"\n",
        "        ),\n",
        "        gr.components.Slider(\n",
        "            minimum=0.1, maximum=10.0, step=0.1, value=2.0, label=\"Repetition Penalty\"\n",
        "        ),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.inputs.Textbox(\n",
        "            lines=25,\n",
        "            label=\"Output\",\n",
        "        )\n",
        "    ],\n",
        "    title=\"Chinese-Vicuna 中文小羊驼\",\n",
        "    description=\"中文小羊驼由各种高质量的开源instruction数据集，结合Alpaca-lora的代码训练而来，模型基于开源的llama7B，主要贡献是对应的lora模型。由于代码训练资源要求较小，希望为llama中文lora社区做一份贡献。\",\n",
        ").queue().launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "SQlel-0xXFj9",
        "outputId": "e1233cd6-2f4d-422c-b2c8-d3baea5baeaa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gradio/inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://353f2295b7aafdccd3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://353f2295b7aafdccd3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hahaha\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcdLMRlCWjTx",
        "outputId": "6afd1b52-8ec4-4807-9ee4-9ef311a4881c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hahaha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果你生成的网页信息被模型加载信息覆盖了，可以再跑一遍代码\n",
        "\n",
        "If the information on your generated page is overwritten by the model loading information, you can run the code again"
      ],
      "metadata": {
        "id": "Nt4R-tr_Z3ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./Chinese-Vicuna/generate.py --model_path decapoda-research/llama-7b-hf --lora_path \"/content/drive/MyDrive/llama_data/checkpoint-200\" --use_local 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoSnXegD_Yph",
        "outputId": "e9eaecff-2de6-4b26-c482-0b9d80d06545"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/./Chinese-Vicuna/generate.py\", line 3, in <module>\n",
            "    from peft import PeftModel, PeftModelForCausalLM, LoraConfig\n",
            "ModuleNotFoundError: No module named 'peft'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tPY_xYF3qr9",
        "outputId": "b4762c0e-5f8d-4021-bb2e-29f6e338c4f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch                         1.13.1+cu116\n",
            "torchaudio                    0.13.1+cu116\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.14.1\n",
            "torchvision                   0.14.1+cu116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whereis python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPuMKP7X3voY",
        "outputId": "ed18ff78-a79d-4011-d8da-bc4654c8f41f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python: /usr/bin/python2.7 /usr/bin/python3.8 /usr/bin/python3.8-config /usr/bin/python3.9 /usr/bin/python3.9-config /usr/lib/python2.7 /usr/lib/python3.8 /usr/lib/python3.9 /etc/python2.7 /etc/python3.8 /etc/python3.9 /usr/local/bin/python /usr/local/lib/python3.9 /usr/local/lib/python2.7 /usr/local/lib/python3.8 /usr/include/python3.8 /usr/include/python3.9\n"
          ]
        }
      ]
    }
  ]
}